{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%config IPCompleter.greedy = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import pprint\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Load NLTK and Test Dataset</font>\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading Treebank tagged sentences using **universal** tagset\n",
    "\n",
    "`VERB - verbs (all tenses and modes)\n",
    "NOUN - nouns (common and proper)\n",
    "PRON - pronouns \n",
    "ADJ - adjectives\n",
    "ADV - adverbs\n",
    "ADP - adpositions (prepositions and postpositions)\n",
    "CONJ - conjunctions\n",
    "DET - determiners\n",
    "NUM - cardinal numbers\n",
    "PRT - particles or other function words\n",
    "X - other: foreign words, typos, abbreviations\n",
    ". - punctuation\n",
    "`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk_data = list(nltk.corpus.treebank.tagged_sents(tagset='universal'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[('Pierre', 'NOUN'), ('Vinken', 'NOUN'), (',', '.'), ('61', 'NUM'), ('years', 'NOUN'), ('old', 'ADJ'), (',', '.'), ('will', 'VERB'), ('join', 'VERB'), ('the', 'DET'), ('board', 'NOUN'), ('as', 'ADP'), ('a', 'DET'), ('nonexecutive', 'ADJ'), ('director', 'NOUN'), ('Nov.', 'NOUN'), ('29', 'NUM'), ('.', '.')], [('Mr.', 'NOUN'), ('Vinken', 'NOUN'), ('is', 'VERB'), ('chairman', 'NOUN'), ('of', 'ADP'), ('Elsevier', 'NOUN'), ('N.V.', 'NOUN'), (',', '.'), ('the', 'DET'), ('Dutch', 'NOUN'), ('publishing', 'VERB'), ('group', 'NOUN'), ('.', '.')]]\n"
     ]
    }
   ],
   "source": [
    "# observe a few tagged sentences from the corpora\n",
    "print(nltk_data[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Android is a mobile operating system developed by Google.\\nAndroid has been the best-selling OS worldwide on smartphones since 2011 and on tablets since 2013.\\nGoogle and Twitter made a deal in 2015 that gave Google access to Twitter's firehose.\\nTwitter is an online news and social networking service on which users post and interact with messages known as tweets.\\nBefore entering politics, Donald Trump was a domineering businessman and a television personality.\\nThe 2018 FIFA World Cup is the 21st FIFA World Cup, an international football tournament contested once every four years.\\nThis is the first World Cup to be held in Eastern Europe and the 11th time that it has been held in Europe.\\nShow me the cheapest round trips from Dallas to Atlanta\\nI would like to see flights from Denver to Philadelphia.\\nShow me the price of the flights leaving Atlanta at about 3 in the afternoon and arriving in San Francisco.\\nNASA invited social media users to experience the launch of ICESAT-2 Satellite.\\n\\n\\n\\n\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_object = open(r\"test-sentences.txt\",\"r\", encoding=\"latin1\")\n",
    "test_data = file_object.read()\n",
    "test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "181"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# number of words in the test dataset\n",
    "test_data_words = nltk.word_tokenize(test_data)\n",
    "len(test_data_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tagging Test Dataset With NLTK POS Tagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'words with tagged with .'\n",
      "[',', '.']\n"
     ]
    }
   ],
   "source": [
    "test_tagged_words = {}\n",
    "test_tagged = nltk.pos_tag(test_data_words, tagset='universal')\n",
    "universal_tagset = [\n",
    "    'VERB', 'NOUN', 'PRON', 'ADJ', 'ADV', \n",
    "    'ADP', 'CONJ', 'DET', 'NUM', 'PRT', 'X', '.'\n",
    "]\n",
    "\n",
    "for utag in universal_tagset:\n",
    "    test_tagged_words[utag] = sorted(\n",
    "        set([word for (word, tag) in test_tagged if tag == utag]))\n",
    "\n",
    "i = random.randrange(len(universal_tagset))\n",
    "\n",
    "pprint.pprint('words with tagged with {}'.format(universal_tagset[i]))\n",
    "pprint.pprint(test_tagged_words[universal_tagset[i]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Split data into train and validation datasets\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sentences in train dataset : 3718\n",
      "Number of sentences in validation dataset : 196\n"
     ]
    }
   ],
   "source": [
    "train_set, validation_set = train_test_split(nltk_data,\n",
    "                                             test_size=0.05,\n",
    "                                             random_state=1234)\n",
    "\n",
    "print('Number of sentences in train dataset : {0}'.format(len(train_set)))\n",
    "print('Number of sentences in validation dataset : {0}'.format(len(validation_set)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tagged_words = [tup for sent in train_set for tup in sent]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total number of words in the training set : 95799\n",
      "total number of unique words in the training set: 12073\n"
     ]
    }
   ],
   "source": [
    "tokens = [pair[0] for pair in train_tagged_words]\n",
    "print('total number of words in the training set : {0}'.format(len(tokens)))\n",
    "\n",
    "vocabulary = set(tokens)\n",
    "print('total number of unique words in the training set: {0}'.format(len(vocabulary)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of tags in the universal tagset : 12\n",
      "['.', 'ADJ', 'ADP', 'ADV', 'CONJ', 'DET', 'NOUN', 'NUM', 'PRON', 'PRT', 'VERB', 'X']\n"
     ]
    }
   ],
   "source": [
    "all_tags = [pair[1] for pair in train_tagged_words]\n",
    "unique_tags = sorted(set(all_tags))\n",
    "\n",
    "print('number of tags in the universal tagset : {}'.format(len(unique_tags)))\n",
    "print(unique_tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Helper Functions\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Store number of times a tag 'T' appears in the training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'.': 11130, 'ADJ': 6063, 'ADP': 9387, 'ADV': 3052, 'CONJ': 2144, 'DET': 8269, 'NOUN': 27471, 'NUM': 3364, 'PRON': 2619, 'PRT': 3070, 'VERB': 12910, 'X': 6320}\n"
     ]
    }
   ],
   "source": [
    "tag_count_dict = dict()\n",
    "\n",
    "for utag in unique_tags:\n",
    "    tag_list = [pair[1] for pair in train_tagged_words if pair[1] == utag]\n",
    "    tag_count_dict[utag] = len(tag_list)\n",
    "    \n",
    "print(tag_count_dict)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### List of Unknown Words in Validation Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of unknown words in validation data set : 335\n"
     ]
    }
   ],
   "source": [
    "val_data_unknown_words = [word for sent in validation_set for (word, tag) in sent if word not in vocabulary]\n",
    "print('number of unknown words in validation data set : {0}'.format(len(set(val_data_unknown_words))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### List of Unknown Words in Test Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of unknown words in test data set : 28\n"
     ]
    }
   ],
   "source": [
    "test_data_unknown_words = [word for word in test_data_words if word not in vocabulary]\n",
    "print('number of unknown words in test data set : {0}'.format(len(set(test_data_unknown_words))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate Number of Words correctly tagged in Test Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_test_dataset_accuracy(tagged_test_set):\n",
    "    total_words = 0\n",
    "    correct_tagged_words = 0\n",
    "\n",
    "    for word, tag in tagged_test_set:\n",
    "        try:\n",
    "            list_for_tag = test_tagged_words[tag]\n",
    "        except KeyError:\n",
    "            list_for_tag = []\n",
    "\n",
    "        total_words += 1\n",
    "\n",
    "        if word in list_for_tag:\n",
    "            correct_tagged_words += 1\n",
    "\n",
    "    print('total words - {0}. correctly tagged words - {1}. accuracy - {2}'.\n",
    "          format(total_words, correct_tagged_words,\n",
    "                 correct_tagged_words / total_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Learning HMM Model Parameters\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Emission Probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_given_tag(word, tag, train_bag=train_tagged_words):\n",
    "\n",
    "    w_given_tag_list = [\n",
    "        pair[0] for pair in train_bag if pair[0] == word and pair[1] == tag\n",
    "    ]\n",
    "    count_w_given_tag = len(w_given_tag_list)\n",
    "\n",
    "    return count_w_given_tag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transition Probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def t2_given_t1(t2, t1, train_bag=train_tagged_words):\n",
    "    \n",
    "    count_t2_t1 = 0\n",
    "\n",
    "    for index in range(len(all_tags) - 1):\n",
    "        if all_tags[index] == t1 and all_tags[index + 1] == t2:\n",
    "            count_t2_t1 += 1\n",
    "\n",
    "    return count_t2_t1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "tags_matrix = np.zeros((len(unique_tags), len(unique_tags)), dtype='float32')\n",
    "\n",
    "for i, t1 in enumerate(list(unique_tags)):\n",
    "    for j, t2 in enumerate(list(unique_tags)):\n",
    "        count_t1 = tag_count_dict[t1]\n",
    "        tags_matrix[i, j] = t2_given_t1(t2, t1) / count_t1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>.</th>\n",
       "      <th>ADJ</th>\n",
       "      <th>ADP</th>\n",
       "      <th>ADV</th>\n",
       "      <th>CONJ</th>\n",
       "      <th>DET</th>\n",
       "      <th>NOUN</th>\n",
       "      <th>NUM</th>\n",
       "      <th>PRON</th>\n",
       "      <th>PRT</th>\n",
       "      <th>VERB</th>\n",
       "      <th>X</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>.</th>\n",
       "      <td>0.094070</td>\n",
       "      <td>0.044654</td>\n",
       "      <td>0.090386</td>\n",
       "      <td>0.051932</td>\n",
       "      <td>0.057772</td>\n",
       "      <td>0.173226</td>\n",
       "      <td>0.223091</td>\n",
       "      <td>0.080593</td>\n",
       "      <td>0.065768</td>\n",
       "      <td>0.002336</td>\n",
       "      <td>0.088769</td>\n",
       "      <td>0.027314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ADJ</th>\n",
       "      <td>0.065809</td>\n",
       "      <td>0.065314</td>\n",
       "      <td>0.077519</td>\n",
       "      <td>0.004948</td>\n",
       "      <td>0.016658</td>\n",
       "      <td>0.004948</td>\n",
       "      <td>0.698499</td>\n",
       "      <td>0.021112</td>\n",
       "      <td>0.000660</td>\n",
       "      <td>0.010886</td>\n",
       "      <td>0.012205</td>\n",
       "      <td>0.021442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ADP</th>\n",
       "      <td>0.039842</td>\n",
       "      <td>0.105785</td>\n",
       "      <td>0.016512</td>\n",
       "      <td>0.013849</td>\n",
       "      <td>0.000959</td>\n",
       "      <td>0.322893</td>\n",
       "      <td>0.322893</td>\n",
       "      <td>0.062001</td>\n",
       "      <td>0.070203</td>\n",
       "      <td>0.001491</td>\n",
       "      <td>0.008522</td>\n",
       "      <td>0.035048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ADV</th>\n",
       "      <td>0.134666</td>\n",
       "      <td>0.129751</td>\n",
       "      <td>0.118611</td>\n",
       "      <td>0.081258</td>\n",
       "      <td>0.006881</td>\n",
       "      <td>0.068480</td>\n",
       "      <td>0.031127</td>\n",
       "      <td>0.031455</td>\n",
       "      <td>0.015400</td>\n",
       "      <td>0.014744</td>\n",
       "      <td>0.344364</td>\n",
       "      <td>0.023263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CONJ</th>\n",
       "      <td>0.033116</td>\n",
       "      <td>0.118937</td>\n",
       "      <td>0.052705</td>\n",
       "      <td>0.055970</td>\n",
       "      <td>0.000466</td>\n",
       "      <td>0.118470</td>\n",
       "      <td>0.348881</td>\n",
       "      <td>0.041511</td>\n",
       "      <td>0.057369</td>\n",
       "      <td>0.005131</td>\n",
       "      <td>0.158582</td>\n",
       "      <td>0.008862</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DET</th>\n",
       "      <td>0.017777</td>\n",
       "      <td>0.203652</td>\n",
       "      <td>0.009191</td>\n",
       "      <td>0.012698</td>\n",
       "      <td>0.000484</td>\n",
       "      <td>0.005442</td>\n",
       "      <td>0.638650</td>\n",
       "      <td>0.022373</td>\n",
       "      <td>0.003749</td>\n",
       "      <td>0.000242</td>\n",
       "      <td>0.039545</td>\n",
       "      <td>0.046197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NOUN</th>\n",
       "      <td>0.239307</td>\n",
       "      <td>0.012231</td>\n",
       "      <td>0.177023</td>\n",
       "      <td>0.017182</td>\n",
       "      <td>0.042263</td>\n",
       "      <td>0.013250</td>\n",
       "      <td>0.264898</td>\n",
       "      <td>0.009537</td>\n",
       "      <td>0.004769</td>\n",
       "      <td>0.043974</td>\n",
       "      <td>0.146336</td>\n",
       "      <td>0.029231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NUM</th>\n",
       "      <td>0.115933</td>\n",
       "      <td>0.032402</td>\n",
       "      <td>0.035672</td>\n",
       "      <td>0.002973</td>\n",
       "      <td>0.013377</td>\n",
       "      <td>0.002973</td>\n",
       "      <td>0.354637</td>\n",
       "      <td>0.184899</td>\n",
       "      <td>0.001486</td>\n",
       "      <td>0.027051</td>\n",
       "      <td>0.018133</td>\n",
       "      <td>0.210464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PRON</th>\n",
       "      <td>0.040473</td>\n",
       "      <td>0.073692</td>\n",
       "      <td>0.023291</td>\n",
       "      <td>0.032837</td>\n",
       "      <td>0.004582</td>\n",
       "      <td>0.009164</td>\n",
       "      <td>0.207331</td>\n",
       "      <td>0.007255</td>\n",
       "      <td>0.007637</td>\n",
       "      <td>0.011837</td>\n",
       "      <td>0.487972</td>\n",
       "      <td>0.093929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PRT</th>\n",
       "      <td>0.041694</td>\n",
       "      <td>0.084039</td>\n",
       "      <td>0.021173</td>\n",
       "      <td>0.009772</td>\n",
       "      <td>0.002280</td>\n",
       "      <td>0.099674</td>\n",
       "      <td>0.247883</td>\n",
       "      <td>0.056678</td>\n",
       "      <td>0.017915</td>\n",
       "      <td>0.001954</td>\n",
       "      <td>0.402932</td>\n",
       "      <td>0.014007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>VERB</th>\n",
       "      <td>0.035167</td>\n",
       "      <td>0.065221</td>\n",
       "      <td>0.091402</td>\n",
       "      <td>0.083501</td>\n",
       "      <td>0.005577</td>\n",
       "      <td>0.133617</td>\n",
       "      <td>0.110844</td>\n",
       "      <td>0.022696</td>\n",
       "      <td>0.035321</td>\n",
       "      <td>0.031216</td>\n",
       "      <td>0.167622</td>\n",
       "      <td>0.217816</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>X</th>\n",
       "      <td>0.162816</td>\n",
       "      <td>0.016456</td>\n",
       "      <td>0.144937</td>\n",
       "      <td>0.026108</td>\n",
       "      <td>0.010759</td>\n",
       "      <td>0.054114</td>\n",
       "      <td>0.062184</td>\n",
       "      <td>0.002690</td>\n",
       "      <td>0.056329</td>\n",
       "      <td>0.184652</td>\n",
       "      <td>0.204114</td>\n",
       "      <td>0.074842</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             .       ADJ       ADP       ADV      CONJ       DET      NOUN  \\\n",
       ".     0.094070  0.044654  0.090386  0.051932  0.057772  0.173226  0.223091   \n",
       "ADJ   0.065809  0.065314  0.077519  0.004948  0.016658  0.004948  0.698499   \n",
       "ADP   0.039842  0.105785  0.016512  0.013849  0.000959  0.322893  0.322893   \n",
       "ADV   0.134666  0.129751  0.118611  0.081258  0.006881  0.068480  0.031127   \n",
       "CONJ  0.033116  0.118937  0.052705  0.055970  0.000466  0.118470  0.348881   \n",
       "DET   0.017777  0.203652  0.009191  0.012698  0.000484  0.005442  0.638650   \n",
       "NOUN  0.239307  0.012231  0.177023  0.017182  0.042263  0.013250  0.264898   \n",
       "NUM   0.115933  0.032402  0.035672  0.002973  0.013377  0.002973  0.354637   \n",
       "PRON  0.040473  0.073692  0.023291  0.032837  0.004582  0.009164  0.207331   \n",
       "PRT   0.041694  0.084039  0.021173  0.009772  0.002280  0.099674  0.247883   \n",
       "VERB  0.035167  0.065221  0.091402  0.083501  0.005577  0.133617  0.110844   \n",
       "X     0.162816  0.016456  0.144937  0.026108  0.010759  0.054114  0.062184   \n",
       "\n",
       "           NUM      PRON       PRT      VERB         X  \n",
       ".     0.080593  0.065768  0.002336  0.088769  0.027314  \n",
       "ADJ   0.021112  0.000660  0.010886  0.012205  0.021442  \n",
       "ADP   0.062001  0.070203  0.001491  0.008522  0.035048  \n",
       "ADV   0.031455  0.015400  0.014744  0.344364  0.023263  \n",
       "CONJ  0.041511  0.057369  0.005131  0.158582  0.008862  \n",
       "DET   0.022373  0.003749  0.000242  0.039545  0.046197  \n",
       "NOUN  0.009537  0.004769  0.043974  0.146336  0.029231  \n",
       "NUM   0.184899  0.001486  0.027051  0.018133  0.210464  \n",
       "PRON  0.007255  0.007637  0.011837  0.487972  0.093929  \n",
       "PRT   0.056678  0.017915  0.001954  0.402932  0.014007  \n",
       "VERB  0.022696  0.035321  0.031216  0.167622  0.217816  \n",
       "X     0.002690  0.056329  0.184652  0.204114  0.074842  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tag = pd.DataFrame(tags_matrix,\n",
    "                      columns=list(unique_tags),\n",
    "                      index=list(unique_tags))\n",
    "\n",
    "df_tag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Start Probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       ".       0.094070\n",
       "ADJ     0.044654\n",
       "ADP     0.090386\n",
       "ADV     0.051932\n",
       "CONJ    0.057772\n",
       "DET     0.173226\n",
       "NOUN    0.223091\n",
       "NUM     0.080593\n",
       "PRON    0.065768\n",
       "PRT     0.002336\n",
       "VERB    0.088769\n",
       "X       0.027314\n",
       "Name: ., dtype: float32"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tag.loc['.', :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Vanilla Viterbi Based POS Tagger\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Viterbi_Vanilla(words, train_bag=train_tagged_words):\n",
    "    state = []\n",
    "    \n",
    "    for key, word in enumerate(words):\n",
    "        #initialise list of probability column for a given observation\n",
    "        p = []\n",
    "        for tag in unique_tags:\n",
    "            if key == 0:\n",
    "                transition_p = df_tag.loc['.', tag]\n",
    "            else:\n",
    "                transition_p = df_tag.loc[state[-1], tag]\n",
    "\n",
    "            # compute emission and state probabilities\n",
    "            emission_p = word_given_tag(words[key], tag) / tag_count_dict[tag]\n",
    "            state_probability = emission_p * transition_p\n",
    "            \n",
    "            p.append(state_probability)\n",
    "\n",
    "        pmax = max(p)\n",
    "        \n",
    "        # getting state for which probability is maximum\n",
    "        # tagging unknown words as 'X' to mark those as foreign words\n",
    "        if pmax == 0:\n",
    "            state_max = 'X'\n",
    "        else:    \n",
    "            state_max = unique_tags[p.index(pmax)]\n",
    "\n",
    "        state.append(state_max)\n",
    "\n",
    "    return list(zip(words, state))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Running Algorithm On Validation Dataset\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(1234)\n",
    "\n",
    "random_indices = [random.randint(1, len(validation_set)) for x in range(5)]\n",
    "\n",
    "validation_run = [validation_set[i] for i in random_indices]\n",
    "\n",
    "validation_run_base = [tup for sent in validation_run for tup in sent]\n",
    "\n",
    "validation_untagged_words = [tup[0] for tup in validation_run_base]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of words in selected validation set : 166\n"
     ]
    }
   ],
   "source": [
    "print('number of words in selected validation set : {0}'.format(len(validation_untagged_words)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 28.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "validation_tagged_sent = Viterbi_Vanilla(validation_untagged_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Model Validation\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8674698795180723"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correct_tags = [i for i, j in zip(validation_run_base, validation_tagged_sent) if i == j]\n",
    "\n",
    "accuracy = len(correct_tags) / len(validation_run_base)\n",
    "\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(('sell', 'NOUN'), ('sell', 'VERB')),\n",
       " (('printers', 'NOUN'), ('printers', 'X')),\n",
       " (('there', 'ADV'), ('there', 'DET')),\n",
       " (('Gunmen', 'NOUN'), ('Gunmen', 'X')),\n",
       " (('Lebanon', 'NOUN'), ('Lebanon', 'X')),\n",
       " (('assassinated', 'VERB'), ('assassinated', 'X')),\n",
       " (('Arabian', 'NOUN'), ('Arabian', 'X')),\n",
       " (('pro-Iranian', 'ADJ'), ('pro-Iranian', 'X')),\n",
       " (('Islamic', 'NOUN'), ('Islamic', 'X')),\n",
       " (('slaying', 'NOUN'), ('slaying', 'X')),\n",
       " (('avenge', 'VERB'), ('avenge', 'X')),\n",
       " (('beheading', 'NOUN'), ('beheading', 'X')),\n",
       " (('terrorists', 'NOUN'), ('terrorists', 'X')),\n",
       " (('Riyadh', 'NOUN'), ('Riyadh', 'X')),\n",
       " (('Card', 'NOUN'), ('Card', 'X')),\n",
       " (('sweepstakes', 'NOUN'), ('sweepstakes', 'X')),\n",
       " (('forthcoming', 'ADJ'), ('forthcoming', 'X')),\n",
       " (('10-year', 'NUM'), ('10-year', 'ADJ')),\n",
       " (('yen-denominated', 'ADJ'), ('yen-denominated', 'X')),\n",
       " (('about', 'ADV'), ('about', 'ADP')),\n",
       " (('redeeming', 'VERB'), ('redeeming', 'X')),\n",
       " (('convert', 'VERB'), ('convert', 'X'))]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation_incorrect_tagged_words = [(i, j) for i, j in zip(validation_run_base, validation_tagged_sent) if i != j]\n",
    "\n",
    "print(len(validation_incorrect_tagged_words))\n",
    "validation_incorrect_tagged_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> -  *Accuracy of Vanilla Viterbi - in **high 80s** depending on validation dataset*\n",
    "-  *Number of incorrect tagged words from validation set : **22** words*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Running Algorithm On Test Dataset\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 36.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "test_tagged_set = Viterbi_Vanilla(test_data_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate Test Dataset Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total words - 181. correctly tagged words - 139. accuracy - 0.7679558011049724\n"
     ]
    }
   ],
   "source": [
    "calc_test_dataset_accuracy(test_tagged_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Android', 'X'), ('Google', 'X'), ('Android', 'X'), ('OS', 'X'), ('worldwide', 'X'), ('smartphones', 'X'), ('2011', 'X'), ('2013', 'X'), ('Google', 'X'), ('Twitter', 'X'), ('2015', 'X'), ('Google', 'X'), ('Twitter', 'X'), ('firehose', 'X'), ('Twitter', 'X'), ('online', 'X'), ('interact', 'X'), ('messages', 'X'), ('tweets', 'X'), ('domineering', 'X'), ('personality', 'X'), ('2018', 'X'), ('FIFA', 'X'), ('Cup', 'X'), ('21st', 'X'), ('FIFA', 'X'), ('Cup', 'X'), ('tournament', 'X'), ('contested', 'X'), ('Cup', 'X'), ('trips', 'X'), ('arriving', 'X'), ('NASA', 'X'), ('invited', 'X'), ('ICESAT-2', 'X'), ('Satellite', 'X')]\n"
     ]
    }
   ],
   "source": [
    "test_unknown_tagged_words = [tup for tup in test_tagged_set if tup[0] in test_data_unknown_words]\n",
    "\n",
    "print(test_unknown_tagged_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> -  *all unknown words have been assigned the <b>1st tag</b> present in the universal tagset*\n",
    "-  *all unknown proper nouns like <b>Android</b> and <b>Google</b> are incorrectly tagged*\n",
    "-  *all unknown numbers like <b>2013</b> and <b>2015</b> are incorrectly tagged*\n",
    "-  *all unknown verbs like <b>contested</b> and <b>arriving</b> are incorrectly tagged*\n",
    "\n",
    "> -  *overall accuracy obtained on test data set : <b>76.79%</b>*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solve Unknown Words Problem\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 1 : Combine Viterbi With Trigram State\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> For an unknown word, use the following to compute a tag : \n",
    "-  calculate maximum likelihood estimate of a transition probability of :\n",
    "     -  unigram tags\n",
    "     -  bigram tags i.e. P(t2 | t1) = C(t1, t2) / C(t1)\n",
    "     -  trigram tags i.e. P(t3 | t2, t1) = C(t1, t2, t3) / C(t1, t2)\n",
    "-  if unknown word is the first word of a sentence, use **start probability**\n",
    "-  if unknown word is the second word of a sentence, use **bigram probability**\n",
    "-  if unknown word is present in any other position of a sentence, use **trigram probability**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Calculate Tag Sequences & Probabilities\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate 'n' gram tag sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> -  Sample output when n = 1 \n",
    "    -  `('.',), ('DET',), ('NOUN',), ('ADP',), ('ADJ',), ('NOUN',)`\n",
    "-  Sample output when n = 2\n",
    "   -  `('NOUN', 'ADJ'), ('ADJ', 'CONJ'), ('CONJ', 'ADP'), ('ADP', 'NUM')`\n",
    "-  Sameple output when n = 3\n",
    "   -  `('VERB', 'ADP', 'DET'), ('ADP', 'DET', 'ADJ'), ('DET', 'ADJ', 'NOUN')`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_n_tag_seq(input_tags, n):\n",
    "    tag_seq = [tuple(input_tags[i:(i + n)]) for i in range(len(input_tags) - n + 1)]\n",
    "    return tag_seq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate 'n' gram tag counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> -  Sample output for unigram tags \n",
    "    -  `('.',): 10611, ('DET',): 7047, ('NOUN',): 24722, ('ADP',): 8745`\n",
    "-  Sample output for bigram tags\n",
    "   -  `('X', 'ADP'): 915, ('.', 'VERB'): 950, ('VERB', 'DET'): 1723, ('NOUN', 'X'): 799`\n",
    "-  Sameple output for trigram tags\n",
    "   -  `('DET', 'NOUN', '.'): 862, ('NOUN', '.', 'DET'): 711, ('.', 'DET', 'NOUN'): 592`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_tag_seq_counts(sent_tags):\n",
    "    unigram_dict = defaultdict(int)\n",
    "    bigram_dict = defaultdict(int)\n",
    "    trigram_dict = defaultdict(int)\n",
    "\n",
    "    unigram_tags = generate_n_tag_seq(sent_tags[2:], 1)\n",
    "    bigram_tags = generate_n_tag_seq(sent_tags[1:], 2)\n",
    "    trigram_tags = generate_n_tag_seq(sent_tags, 3)\n",
    "    \n",
    "    for unigram in unigram_tags:\n",
    "        unigram_dict[unigram] += 1\n",
    "\n",
    "    for bigram in bigram_tags:\n",
    "        bigram_dict[bigram] += 1\n",
    "\n",
    "    for trigram in trigram_tags:\n",
    "        trigram_dict[trigram] += 1\n",
    "\n",
    "    return unigram_dict, bigram_dict, trigram_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate 'n' gram tag probability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Returns unigram, bigram and trigram tag sequence probabilities in 3 separate dictionary objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_tag_seq_prob(sent_tags, unigram_dict, bigram_dict, trigram_dict):\n",
    "    unigram_prob_dict = defaultdict(int)\n",
    "    bigram_prob_dict = defaultdict(int)\n",
    "    trigram_prob_dict = defaultdict(int)\n",
    "\n",
    "    unigram_total = sum(unigram_dict.values())\n",
    "\n",
    "    unigram_prob_dict = {a: (unigram_dict[a] / unigram_total) for a in unigram_dict}\n",
    "\n",
    "    bigram_prob_dict = {(a, b): (bigram_dict[(a, b)] / unigram_dict[(a, )])\n",
    "                        for a, b in bigram_dict}\n",
    "    \n",
    "    trigram_prob_dict = {(a, b, c): (trigram_dict[(a, b, c)] / bigram_dict[(a, b)])\n",
    "                         for a, b, c in trigram_dict}\n",
    "    \n",
    "    return unigram_prob_dict, bigram_prob_dict, trigram_prob_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate 'n' gram tag probabilities\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "unigram_dict, bigram_dict, trigram_dict = generate_tag_seq_counts(all_tags)\n",
    "\n",
    "unigram_prob_dict, bigram_prob_dict, trigram_prob_dict = generate_tag_seq_prob(\n",
    "    all_tags, unigram_dict, bigram_dict, trigram_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate which tag has the maximum unigram probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'NOUN'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prob2tag = {v: k for k, v in unigram_prob_dict.items()}\n",
    "max_prob = max(unigram_prob_dict.values())\n",
    "\n",
    "tag_max_uni_prob = prob2tag[max_prob]\n",
    "tag_max_uni_prob[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate most likely tag 't2' that a given tag 't1' will transition to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(None,\n",
       "            {'.': 'NOUN',\n",
       "             'ADJ': 'NOUN',\n",
       "             'ADP': 'DET',\n",
       "             'ADV': 'VERB',\n",
       "             'CONJ': 'NOUN',\n",
       "             'DET': 'NOUN',\n",
       "             'NOUN': 'NOUN',\n",
       "             'NUM': 'NOUN',\n",
       "             'PRON': 'VERB',\n",
       "             'PRT': 'VERB',\n",
       "             'VERB': 'X',\n",
       "             'X': 'VERB'})"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tag_big_pair = defaultdict()\n",
    "\n",
    "for tag in unique_tags:\n",
    "    bigram_prob_2_tag = {\n",
    "        v: k\n",
    "        for k, v in bigram_prob_dict.items() if k[0] == tag\n",
    "    }\n",
    "    max_prob = max(bigram_prob_2_tag.keys())\n",
    "\n",
    "    max_bigram_prob_tup = bigram_prob_2_tag[max_prob]\n",
    "\n",
    "    tag_big_pair[max_bigram_prob_tup[0]] = max_bigram_prob_tup[1]\n",
    "\n",
    "tag_big_pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_t3_given_t2_and_t1(t1, t2):\n",
    "    max_prob = 0\n",
    "\n",
    "    for tag in unique_tags:\n",
    "        try:\n",
    "            v = trigram_prob_dict[(t1, t2, tag)]\n",
    "            if v > max_prob:\n",
    "                max_prob = v\n",
    "                selected_tag = (t1, t2, tag)\n",
    "        except KeyError:\n",
    "            v = 0\n",
    "    \n",
    "    return selected_tag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Define Modified Viterbi With Trigram Tagger\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trigram_tagger(state, word):\n",
    "    current_state_len = len(state)\n",
    "    word_tag = ''\n",
    "    \n",
    "    if current_state_len == 0:\n",
    "        # use unigram transition probbaility at the start of a sentence         \n",
    "        word_tag = tag_max_uni_prob[0]\n",
    "    elif current_state_len == 1:\n",
    "        # use bigram transition probability for 2nd word of a sentence\n",
    "        word_tag = tag_big_pair[state[-1]]\n",
    "    else:\n",
    "        # use tigram transition probability for all other words\n",
    "        selected_tag = generate_t3_given_t2_and_t1(state[-2], state[-1])\n",
    "        \n",
    "        # special case for end of sentence\n",
    "        # fallback to bigram transition probability\n",
    "        if selected_tag[2] == '.' and word != '.':\n",
    "            word_tag = tag_big_pair[state[-1]]\n",
    "        else:\n",
    "            word_tag = selected_tag[2]\n",
    "\n",
    "    return word_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Viterbi_With_Trigram_State(words, train_bag=train_tagged_words):\n",
    "    state = []\n",
    "\n",
    "    for key, word in enumerate(words):\n",
    "        p = []\n",
    "        for tag in unique_tags:\n",
    "            if key == 0:\n",
    "                transition_p = df_tag.loc['.', tag]\n",
    "            else:\n",
    "                if key == 0:\n",
    "                    transition_p = df_tag.loc['.', tag]\n",
    "                else:\n",
    "                    transition_p = df_tag.loc[state[-1], tag]\n",
    "            \n",
    "            emission_p = word_given_tag(words[key], tag) / tag_count_dict[tag]\n",
    "            state_probability = emission_p * transition_p\n",
    "            p.append(state_probability)\n",
    "        \n",
    "        pmax = max(p)\n",
    "\n",
    "        if pmax == 0:\n",
    "            # invoke trigram tagger for unknown words where Viterbi fails to compute a tag\n",
    "            try:\n",
    "                last_end_of_sent = list(reversed(state)).index('.')\n",
    "            except ValueError:\n",
    "                # pass complete state in absence of a period\n",
    "                last_end_of_sent = len(state)\n",
    "                sent_state = state\n",
    "                \n",
    "            if last_end_of_sent == 0:\n",
    "                sent_state = []\n",
    "            else:\n",
    "                sent_state = state[-last_end_of_sent:]\n",
    "\n",
    "            # pass state of current sentence instead of complete state                     \n",
    "            state_max = trigram_tagger(sent_state, word)\n",
    "        else:\n",
    "            state_max = unique_tags[p.index(pmax)]\n",
    "\n",
    "        state.append(state_max)\n",
    "\n",
    "    return list(zip(words, state))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Running Algorithm On Validation Dataset\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 35.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "tagged_seq_trigram = Viterbi_With_Trigram_State(validation_untagged_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Model Validation\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9216867469879518\n"
     ]
    }
   ],
   "source": [
    "check = [i for i, j in zip(tagged_seq_trigram, validation_run_base) if i == j]\n",
    "\n",
    "accuracy = len(check)/len(tagged_seq_trigram)\n",
    "\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[(('sell', 'VERB'), ('sell', 'NOUN'))],\n",
       " [(('printers', 'ADP'), ('printers', 'NOUN'))],\n",
       " [(('start', 'NOUN'), ('start', 'VERB'))],\n",
       " [(('there', 'DET'), ('there', 'ADV'))],\n",
       " [(('assassinated', 'NOUN'), ('assassinated', 'VERB'))],\n",
       " [(('Arabian', 'ADP'), ('Arabian', 'NOUN'))],\n",
       " [(('pro-Iranian', 'NOUN'), ('pro-Iranian', 'ADJ'))],\n",
       " [(('Islamic', 'ADP'), ('Islamic', 'NOUN'))],\n",
       " [(('forthcoming', 'NOUN'), ('forthcoming', 'ADJ'))],\n",
       " [(('10-year', 'ADJ'), ('10-year', 'NUM'))],\n",
       " [(('yen-denominated', 'NOUN'), ('yen-denominated', 'ADJ'))],\n",
       " [(('about', 'ADP'), ('about', 'ADV'))],\n",
       " [(('redeeming', 'NOUN'), ('redeeming', 'VERB'))]]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "incorrect_tagged_cases_trigram = [[j] for i, j in enumerate(zip(tagged_seq_trigram, validation_run_base)) if j[0] != j[1]]\n",
    "\n",
    "print(len(incorrect_tagged_cases_trigram))\n",
    "incorrect_tagged_cases_trigram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> -  *Accuracy of Viterbi With Trigram Tagger - **low-90s** depending on validation dataset*\n",
    "-  *Number of incorrect tagged words from validation set : **13** words*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Running Algorithm On Test Dataset\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 36.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "test_tagged_seq_tigram = Viterbi_With_Trigram_State(test_data_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Android', 'NOUN'), ('is', 'VERB'), ('a', 'DET'), ('mobile', 'ADJ'), ('operating', 'NOUN'), ('system', 'NOUN'), ('developed', 'VERB'), ('by', 'ADP'), ('Google', 'DET'), ('.', '.'), ('Android', 'NOUN'), ('has', 'VERB'), ('been', 'VERB'), ('the', 'DET'), ('best-selling', 'ADJ'), ('OS', 'NOUN'), ('worldwide', 'NOUN'), ('on', 'ADP'), ('smartphones', 'NOUN'), ('since', 'ADP'), ('2011', 'NOUN'), ('and', 'CONJ'), ('on', 'ADP'), ('tablets', 'NOUN'), ('since', 'ADP'), ('2013', 'NOUN'), ('.', '.'), ('Google', 'NOUN'), ('and', 'CONJ'), ('Twitter', 'NOUN'), ('made', 'VERB'), ('a', 'DET'), ('deal', 'NOUN'), ('in', 'ADP'), ('2015', 'NOUN'), ('that', 'ADP'), ('gave', 'VERB'), ('Google', 'NOUN'), ('access', 'NOUN'), ('to', 'PRT'), ('Twitter', 'NOUN'), (\"'s\", 'PRT'), ('firehose', 'NOUN'), ('.', '.'), ('Twitter', 'NOUN'), ('is', 'VERB'), ('an', 'DET'), ('online', 'NOUN'), ('news', 'NOUN'), ('and', 'CONJ'), ('social', 'ADJ'), ('networking', 'NOUN'), ('service', 'NOUN'), ('on', 'ADP'), ('which', 'DET'), ('users', 'NOUN'), ('post', 'NOUN'), ('and', 'CONJ'), ('interact', 'NOUN'), ('with', 'ADP'), ('messages', 'NOUN'), ('known', 'VERB'), ('as', 'ADP'), ('tweets', 'DET'), ('.', '.'), ('Before', 'ADP'), ('entering', 'VERB'), ('politics', 'NOUN'), (',', '.'), ('Donald', 'NOUN'), ('Trump', 'NOUN'), ('was', 'VERB'), ('a', 'DET'), ('domineering', 'NOUN'), ('businessman', 'NOUN'), ('and', 'CONJ'), ('a', 'DET'), ('television', 'NOUN'), ('personality', 'ADP'), ('.', '.'), ('The', 'DET'), ('2018', 'NOUN'), ('FIFA', 'ADP'), ('World', 'NOUN'), ('Cup', 'NOUN'), ('is', 'VERB'), ('the', 'DET'), ('21st', 'NOUN'), ('FIFA', 'ADP'), ('World', 'NOUN'), ('Cup', 'NOUN'), (',', '.'), ('an', 'DET'), ('international', 'ADJ'), ('football', 'NOUN'), ('tournament', 'NOUN'), ('contested', 'NOUN'), ('once', 'ADV'), ('every', 'DET'), ('four', 'NUM'), ('years', 'NOUN'), ('.', '.'), ('This', 'DET'), ('is', 'VERB'), ('the', 'DET'), ('first', 'ADJ'), ('World', 'NOUN'), ('Cup', 'NOUN'), ('to', 'PRT'), ('be', 'VERB'), ('held', 'VERB'), ('in', 'ADP'), ('Eastern', 'NOUN'), ('Europe', 'NOUN'), ('and', 'CONJ'), ('the', 'DET'), ('11th', 'ADJ'), ('time', 'NOUN'), ('that', 'ADP'), ('it', 'PRON'), ('has', 'VERB'), ('been', 'VERB'), ('held', 'VERB'), ('in', 'ADP'), ('Europe', 'NOUN'), ('.', '.'), ('Show', 'NOUN'), ('me', 'PRON'), ('the', 'DET'), ('cheapest', 'ADJ'), ('round', 'NOUN'), ('trips', 'NOUN'), ('from', 'ADP'), ('Dallas', 'NOUN'), ('to', 'PRT'), ('Atlanta', 'NOUN'), ('I', 'PRON'), ('would', 'VERB'), ('like', 'ADP'), ('to', 'PRT'), ('see', 'VERB'), ('flights', 'NOUN'), ('from', 'ADP'), ('Denver', 'NOUN'), ('to', 'PRT'), ('Philadelphia', 'NOUN'), ('.', '.'), ('Show', 'NOUN'), ('me', 'PRON'), ('the', 'DET'), ('price', 'NOUN'), ('of', 'ADP'), ('the', 'DET'), ('flights', 'NOUN'), ('leaving', 'VERB'), ('Atlanta', 'NOUN'), ('at', 'ADP'), ('about', 'ADP'), ('3', 'NUM'), ('in', 'ADP'), ('the', 'DET'), ('afternoon', 'NOUN'), ('and', 'CONJ'), ('arriving', 'NOUN'), ('in', 'ADP'), ('San', 'NOUN'), ('Francisco', 'NOUN'), ('.', '.'), ('NASA', 'NOUN'), ('invited', 'NOUN'), ('social', 'ADJ'), ('media', 'NOUN'), ('users', 'NOUN'), ('to', 'PRT'), ('experience', 'NOUN'), ('the', 'DET'), ('launch', 'NOUN'), ('of', 'ADP'), ('ICESAT-2', 'NOUN'), ('Satellite', 'NOUN'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "print(test_tagged_seq_tigram)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate Test Dataset Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total words - 181. correctly tagged words - 161. accuracy - 0.8895027624309392\n"
     ]
    }
   ],
   "source": [
    "calc_test_dataset_accuracy(test_tagged_seq_tigram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Android', 'NOUN'), ('Google', 'DET'), ('Android', 'NOUN'), ('OS', 'NOUN'), ('worldwide', 'NOUN'), ('smartphones', 'NOUN'), ('2011', 'NOUN'), ('2013', 'NOUN'), ('Google', 'NOUN'), ('Twitter', 'NOUN'), ('2015', 'NOUN'), ('Google', 'NOUN'), ('Twitter', 'NOUN'), ('firehose', 'NOUN'), ('Twitter', 'NOUN'), ('online', 'NOUN'), ('interact', 'NOUN'), ('messages', 'NOUN'), ('tweets', 'DET'), ('domineering', 'NOUN'), ('personality', 'ADP'), ('2018', 'NOUN'), ('FIFA', 'ADP'), ('Cup', 'NOUN'), ('21st', 'NOUN'), ('FIFA', 'ADP'), ('Cup', 'NOUN'), ('tournament', 'NOUN'), ('contested', 'NOUN'), ('Cup', 'NOUN'), ('trips', 'NOUN'), ('arriving', 'NOUN'), ('NASA', 'NOUN'), ('invited', 'NOUN'), ('ICESAT-2', 'NOUN'), ('Satellite', 'NOUN')]\n"
     ]
    }
   ],
   "source": [
    "test_unknown_tagged_words = [tup for tup in test_tagged_seq_tigram if tup[0] in test_data_unknown_words]\n",
    "\n",
    "print(test_unknown_tagged_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> -  *proper nouns have been tagged correctly by this tagger : <b>Android, smartphones, Twitter</b>*\n",
    "-  *unknown words tagged <b>incorrectly</b>*\n",
    " -  *numbers like 2011 & 2013*\n",
    " -  *verbs like domineering & invited*\n",
    " \n",
    "> *overall tagging accuracy improved to <b>88.95%</b>, which includes both known and unknown words*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 2 : Combine Viterbi With Rule Based Tagging\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> For an unknown word, use a rule-based tagger instead of a trigram tagger"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Define Modified Viterbi With Rule Based Tagger\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rule_based_tagger(word):\n",
    "\n",
    "    # define regular expression patterns\n",
    "    patterns = [\n",
    "        (r'.*(ing|ed|es|ould)$', 'VERB'),\n",
    "        (r'.*\\'s$', 'NOUN'),\n",
    "        (r'.*s$', 'NOUN'),\n",
    "        (r'^[A-Z]+.*$', 'NOUN'),\n",
    "        (r'^-?[0-9]+(.[0-9]+)?-?(.*)?$', 'NUM'),\n",
    "        (r'.*', 'NOUN')  # nouns (default)\n",
    "    ]\n",
    "\n",
    "    # use nltk.RegexpTagger\n",
    "    regexp_tagger = nltk.RegexpTagger(patterns)\n",
    "    cal_tag = regexp_tagger.tag([word])\n",
    "    \n",
    "    return cal_tag[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Viterbi_With_Regex(words, train_bag=train_tagged_words):\n",
    "    state = []\n",
    "\n",
    "    for key, word in enumerate(words):\n",
    "        p = []\n",
    "        for tag in unique_tags:\n",
    "            if key == 0:\n",
    "                transition_p = df_tag.loc['.', tag]\n",
    "            else:\n",
    "                transition_p = df_tag.loc[state[-1], tag]\n",
    "\n",
    "            emission_p = word_given_tag(words[key], tag) / tag_count_dict[tag]\n",
    "            state_probability = emission_p * transition_p\n",
    "            p.append(state_probability)\n",
    "\n",
    "        pmax = max(p)\n",
    "\n",
    "        if pmax == 0:\n",
    "            # invoke rule-based tagger for unknown words where Viterbi fails to compute a tag\n",
    "            state_max = rule_based_tagger(word)\n",
    "        else:\n",
    "            state_max = unique_tags[p.index(pmax)]\n",
    "\n",
    "        state.append(state_max)\n",
    "\n",
    "    return list(zip(words, state))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Running Algorithm On Validation Dataset\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 19.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "tagged_seq_regex = Viterbi_With_Regex(validation_untagged_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Model Validation\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.927710843373494\n"
     ]
    }
   ],
   "source": [
    "check = [i for i, j in zip(tagged_seq_regex, validation_run_base) if i == j]\n",
    "\n",
    "accuracy = len(check)/len(tagged_seq_regex)\n",
    "\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[(('sell', 'VERB'), ('sell', 'NOUN'))],\n",
       " [(('there', 'DET'), ('there', 'ADV'))],\n",
       " [(('pro-Iranian', 'NOUN'), ('pro-Iranian', 'ADJ'))],\n",
       " [(('slaying', 'VERB'), ('slaying', 'NOUN'))],\n",
       " [(('avenge', 'NOUN'), ('avenge', 'VERB'))],\n",
       " [(('beheading', 'VERB'), ('beheading', 'NOUN'))],\n",
       " [(('sweepstakes', 'VERB'), ('sweepstakes', 'NOUN'))],\n",
       " [(('forthcoming', 'VERB'), ('forthcoming', 'ADJ'))],\n",
       " [(('10-year', 'ADJ'), ('10-year', 'NUM'))],\n",
       " [(('yen-denominated', 'VERB'), ('yen-denominated', 'ADJ'))],\n",
       " [(('about', 'ADP'), ('about', 'ADV'))],\n",
       " [(('convert', 'NOUN'), ('convert', 'VERB'))]]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "incorrect_tagged_cases_regex = [[j] for i, j in enumerate(zip(tagged_seq_regex, validation_run_base)) if j[0] != j[1]]\n",
    "\n",
    "print(len(incorrect_tagged_cases_regex))\n",
    "incorrect_tagged_cases_regex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> -  *Accuracy of Viterbi With RegexTagger - **mid-90s** depending on validation dataset*\n",
    "-  *Number of incorrect tagged words from validation set : **12** words*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Running Algorithm On Test Dataset\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 23.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "test_tagged_seq_regex = Viterbi_With_Regex(test_data_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Android', 'NOUN'), ('is', 'VERB'), ('a', 'DET'), ('mobile', 'ADJ'), ('operating', 'NOUN'), ('system', 'NOUN'), ('developed', 'VERB'), ('by', 'ADP'), ('Google', 'NOUN'), ('.', '.'), ('Android', 'NOUN'), ('has', 'VERB'), ('been', 'VERB'), ('the', 'DET'), ('best-selling', 'ADJ'), ('OS', 'NOUN'), ('worldwide', 'NOUN'), ('on', 'ADP'), ('smartphones', 'VERB'), ('since', 'ADP'), ('2011', 'NUM'), ('and', 'CONJ'), ('on', 'ADP'), ('tablets', 'NOUN'), ('since', 'ADP'), ('2013', 'NUM'), ('.', '.'), ('Google', 'NOUN'), ('and', 'CONJ'), ('Twitter', 'NOUN'), ('made', 'VERB'), ('a', 'DET'), ('deal', 'NOUN'), ('in', 'ADP'), ('2015', 'NUM'), ('that', 'ADP'), ('gave', 'VERB'), ('Google', 'NOUN'), ('access', 'NOUN'), ('to', 'PRT'), ('Twitter', 'NOUN'), (\"'s\", 'PRT'), ('firehose', 'NOUN'), ('.', '.'), ('Twitter', 'NOUN'), ('is', 'VERB'), ('an', 'DET'), ('online', 'NOUN'), ('news', 'NOUN'), ('and', 'CONJ'), ('social', 'ADJ'), ('networking', 'NOUN'), ('service', 'NOUN'), ('on', 'ADP'), ('which', 'DET'), ('users', 'NOUN'), ('post', 'NOUN'), ('and', 'CONJ'), ('interact', 'NOUN'), ('with', 'ADP'), ('messages', 'VERB'), ('known', 'VERB'), ('as', 'ADP'), ('tweets', 'NOUN'), ('.', '.'), ('Before', 'ADP'), ('entering', 'VERB'), ('politics', 'NOUN'), (',', '.'), ('Donald', 'NOUN'), ('Trump', 'NOUN'), ('was', 'VERB'), ('a', 'DET'), ('domineering', 'VERB'), ('businessman', 'NOUN'), ('and', 'CONJ'), ('a', 'DET'), ('television', 'NOUN'), ('personality', 'NOUN'), ('.', '.'), ('The', 'DET'), ('2018', 'NUM'), ('FIFA', 'NOUN'), ('World', 'NOUN'), ('Cup', 'NOUN'), ('is', 'VERB'), ('the', 'DET'), ('21st', 'NUM'), ('FIFA', 'NOUN'), ('World', 'NOUN'), ('Cup', 'NOUN'), (',', '.'), ('an', 'DET'), ('international', 'ADJ'), ('football', 'NOUN'), ('tournament', 'NOUN'), ('contested', 'VERB'), ('once', 'ADV'), ('every', 'DET'), ('four', 'NUM'), ('years', 'NOUN'), ('.', '.'), ('This', 'DET'), ('is', 'VERB'), ('the', 'DET'), ('first', 'ADJ'), ('World', 'NOUN'), ('Cup', 'NOUN'), ('to', 'PRT'), ('be', 'VERB'), ('held', 'VERB'), ('in', 'ADP'), ('Eastern', 'NOUN'), ('Europe', 'NOUN'), ('and', 'CONJ'), ('the', 'DET'), ('11th', 'ADJ'), ('time', 'NOUN'), ('that', 'ADP'), ('it', 'PRON'), ('has', 'VERB'), ('been', 'VERB'), ('held', 'VERB'), ('in', 'ADP'), ('Europe', 'NOUN'), ('.', '.'), ('Show', 'NOUN'), ('me', 'PRON'), ('the', 'DET'), ('cheapest', 'ADJ'), ('round', 'NOUN'), ('trips', 'NOUN'), ('from', 'ADP'), ('Dallas', 'NOUN'), ('to', 'PRT'), ('Atlanta', 'NOUN'), ('I', 'PRON'), ('would', 'VERB'), ('like', 'ADP'), ('to', 'PRT'), ('see', 'VERB'), ('flights', 'NOUN'), ('from', 'ADP'), ('Denver', 'NOUN'), ('to', 'PRT'), ('Philadelphia', 'NOUN'), ('.', '.'), ('Show', 'NOUN'), ('me', 'PRON'), ('the', 'DET'), ('price', 'NOUN'), ('of', 'ADP'), ('the', 'DET'), ('flights', 'NOUN'), ('leaving', 'VERB'), ('Atlanta', 'NOUN'), ('at', 'ADP'), ('about', 'ADP'), ('3', 'NUM'), ('in', 'ADP'), ('the', 'DET'), ('afternoon', 'NOUN'), ('and', 'CONJ'), ('arriving', 'VERB'), ('in', 'ADP'), ('San', 'NOUN'), ('Francisco', 'NOUN'), ('.', '.'), ('NASA', 'NOUN'), ('invited', 'VERB'), ('social', 'ADJ'), ('media', 'NOUN'), ('users', 'NOUN'), ('to', 'PRT'), ('experience', 'NOUN'), ('the', 'DET'), ('launch', 'NOUN'), ('of', 'ADP'), ('ICESAT-2', 'NOUN'), ('Satellite', 'NOUN'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "print(test_tagged_seq_regex)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check tags assigned to unknown words\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Android', 'NOUN'), ('Google', 'NOUN'), ('Android', 'NOUN'), ('OS', 'NOUN'), ('worldwide', 'NOUN'), ('smartphones', 'VERB'), ('2011', 'NUM'), ('2013', 'NUM'), ('Google', 'NOUN'), ('Twitter', 'NOUN'), ('2015', 'NUM'), ('Google', 'NOUN'), ('Twitter', 'NOUN'), ('firehose', 'NOUN'), ('Twitter', 'NOUN'), ('online', 'NOUN'), ('interact', 'NOUN'), ('messages', 'VERB'), ('tweets', 'NOUN'), ('domineering', 'VERB'), ('personality', 'NOUN'), ('2018', 'NUM'), ('FIFA', 'NOUN'), ('Cup', 'NOUN'), ('21st', 'NUM'), ('FIFA', 'NOUN'), ('Cup', 'NOUN'), ('tournament', 'NOUN'), ('contested', 'VERB'), ('Cup', 'NOUN'), ('trips', 'NOUN'), ('arriving', 'VERB'), ('NASA', 'NOUN'), ('invited', 'VERB'), ('ICESAT-2', 'NOUN'), ('Satellite', 'NOUN')]\n"
     ]
    }
   ],
   "source": [
    "test_unknown_word_tags = [\n",
    "    tag for tag in test_tagged_seq_regex if tag[0] in test_data_unknown_words\n",
    "]\n",
    "\n",
    "print(test_unknown_word_tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate Test Dataset Accuracy\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total words - 181. correctly tagged words - 170. accuracy - 0.9392265193370166\n"
     ]
    }
   ],
   "source": [
    "calc_test_dataset_accuracy(test_tagged_seq_regex)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Unknowns Words Correctly Tagged By Method 2\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> -  *almost all unknown words were tagged correctly with this technique*\n",
    "     -  *<b>Android</b>, <b>Google</b> and <b>Twitter</b> correctly tagged as NOUN*\n",
    "     -  *<b>domineering</b> and <b>invited</b> correctly tagged as VERB*\n",
    "     -  *<b>2011</b>, <b>2013</b> and <b>2015</b> correctly tagged as NUMBER*\n",
    "-  *unknown words tagged incorrectly : smartphones*\n",
    "\n",
    "> *overall tagging accuracy improved further to <b>93.9%</b>, which includes both known and unknown words*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| no | word        | default | trigram | rule based | correct tag |\n",
    "|:--:|-------------|:-------:|:-------:|:----------:|:-----------:|\n",
    "|  1 |   Android   |    X    |   NOUN  |    NOUN    |     NOUN    |\n",
    "|  2 |    Google   |    X    |   DET   |    NOUN    |     NOUN    |\n",
    "|  3 | smartphones |    X    |   NOUN  |    VERB    |     NOUN    |\n",
    "|  4 |     2011    |    X    |   NOUN  |   NUMBER   |    NUMBER   |\n",
    "|  5 |     2015    |    X    |   NOUN  |   NUMBER   |    NUMBER   |\n",
    "|  6 | domineering |    X    |   NOUN  |    VERB    |     VERB    |\n",
    "|  7 |   invited   |    X    |   NOUN  |    VERB    |     VERB    |\n",
    "|  8 |  contested  |    X    |   NOUN  |    VERB    |     VERB    |\n",
    "|  9 |     21st    |    X    |   NOUN  |   NUMBER   |    NUMBER   |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
