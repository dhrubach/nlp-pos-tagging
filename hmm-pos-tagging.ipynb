{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%config IPCompleter.greedy = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import pprint\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Load NLTK and Test Dataset</font>\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading Treebank tagged sentences using **universal** tagset\n",
    "\n",
    "`VERB - verbs (all tenses and modes)\n",
    "NOUN - nouns (common and proper)\n",
    "PRON - pronouns \n",
    "ADJ - adjectives\n",
    "ADV - adverbs\n",
    "ADP - adpositions (prepositions and postpositions)\n",
    "CONJ - conjunctions\n",
    "DET - determiners\n",
    "NUM - cardinal numbers\n",
    "PRT - particles or other function words\n",
    "X - other: foreign words, typos, abbreviations\n",
    ". - punctuation\n",
    "`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk_data = list(nltk.corpus.treebank.tagged_sents(tagset='universal'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[('Pierre', 'NOUN'), ('Vinken', 'NOUN'), (',', '.'), ('61', 'NUM'), ('years', 'NOUN'), ('old', 'ADJ'), (',', '.'), ('will', 'VERB'), ('join', 'VERB'), ('the', 'DET'), ('board', 'NOUN'), ('as', 'ADP'), ('a', 'DET'), ('nonexecutive', 'ADJ'), ('director', 'NOUN'), ('Nov.', 'NOUN'), ('29', 'NUM'), ('.', '.')], [('Mr.', 'NOUN'), ('Vinken', 'NOUN'), ('is', 'VERB'), ('chairman', 'NOUN'), ('of', 'ADP'), ('Elsevier', 'NOUN'), ('N.V.', 'NOUN'), (',', '.'), ('the', 'DET'), ('Dutch', 'NOUN'), ('publishing', 'VERB'), ('group', 'NOUN'), ('.', '.')]]\n"
     ]
    }
   ],
   "source": [
    "# observe a few tagged sentences from the corpora\n",
    "print(nltk_data[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Android is a mobile operating system developed by Google.\\nAndroid has been the best-selling OS worldwide on smartphones since 2011 and on tablets since 2013.\\nGoogle and Twitter made a deal in 2015 that gave Google access to Twitter's firehose.\\nTwitter is an online news and social networking service on which users post and interact with messages known as tweets.\\nBefore entering politics, Donald Trump was a domineering businessman and a television personality.\\nThe 2018 FIFA World Cup is the 21st FIFA World Cup, an international football tournament contested once every four years.\\nThis is the first World Cup to be held in Eastern Europe and the 11th time that it has been held in Europe.\\nShow me the cheapest round trips from Dallas to Atlanta\\nI would like to see flights from Denver to Philadelphia.\\nShow me the price of the flights leaving Atlanta at about 3 in the afternoon and arriving in San Francisco.\\nNASA invited social media users to experience the launch of ICESAT-2 Satellite.\\n\\n\\n\\n\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_object = open(r\"test-sentences.txt\",\"r\", encoding=\"latin1\")\n",
    "test_data = file_object.read()\n",
    "test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "181"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# number of words in the test dataset\n",
    "test_data_words = nltk.word_tokenize(test_data)\n",
    "len(test_data_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tagging Test Dataset With NLTK POS Tagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'words with tagged with ADJ'\n",
      "['best-selling',\n",
      " 'cheapest',\n",
      " 'domineering',\n",
      " 'first',\n",
      " 'international',\n",
      " 'mobile',\n",
      " 'online',\n",
      " 'social']\n"
     ]
    }
   ],
   "source": [
    "test_tagged_words = {}\n",
    "test_tagged = nltk.pos_tag(test_data_words, tagset='universal')\n",
    "universal_tagset = [\n",
    "    'VERB', 'NOUN', 'PRON', 'ADJ', 'ADV', \n",
    "    'ADP', 'CONJ', 'DET', 'NUM', 'PRT', 'X', '.'\n",
    "]\n",
    "\n",
    "for utag in universal_tagset:\n",
    "    test_tagged_words[utag] = sorted(\n",
    "        set([word for (word, tag) in test_tagged if tag == utag]))\n",
    "\n",
    "i = random.randrange(len(universal_tagset))\n",
    "\n",
    "pprint.pprint('words with tagged with {}'.format(universal_tagset[i]))\n",
    "pprint.pprint(test_tagged_words[universal_tagset[i]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Split data into train and validation datasets\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sentences in train dataset : 3718\n",
      "Number of sentences in validation dataset : 196\n"
     ]
    }
   ],
   "source": [
    "train_set, validation_set = train_test_split(nltk_data,\n",
    "                                             test_size=0.05,\n",
    "                                             random_state=1234)\n",
    "\n",
    "print('Number of sentences in train dataset : {0}'.format(len(train_set)))\n",
    "print('Number of sentences in validation dataset : {0}'.format(len(validation_set)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tagged_words = [tup for sent in train_set for tup in sent]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total number of words in the training set : 95799\n",
      "total number of unique words in the training set: 12073\n"
     ]
    }
   ],
   "source": [
    "tokens = [pair[0] for pair in train_tagged_words]\n",
    "print('total number of words in the training set : {0}'.format(len(tokens)))\n",
    "\n",
    "vocabulary = set(tokens)\n",
    "print('total number of unique words in the training set: {0}'.format(len(vocabulary)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of tags in the universal tagset : 12\n",
      "['.', 'ADJ', 'ADP', 'ADV', 'CONJ', 'DET', 'NOUN', 'NUM', 'PRON', 'PRT', 'VERB', 'X']\n"
     ]
    }
   ],
   "source": [
    "all_tags = [pair[1] for pair in train_tagged_words]\n",
    "unique_tags = sorted(set(all_tags))\n",
    "\n",
    "print('number of tags in the universal tagset : {}'.format(len(unique_tags)))\n",
    "print(unique_tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Helper Functions\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Store number of times a tag 'T' appears in the training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'.': 11130, 'ADJ': 6063, 'ADP': 9387, 'ADV': 3052, 'CONJ': 2144, 'DET': 8269, 'NOUN': 27471, 'NUM': 3364, 'PRON': 2619, 'PRT': 3070, 'VERB': 12910, 'X': 6320}\n"
     ]
    }
   ],
   "source": [
    "tag_count_dict = dict()\n",
    "\n",
    "for utag in unique_tags:\n",
    "    tag_list = [pair[1] for pair in train_tagged_words if pair[1] == utag]\n",
    "    tag_count_dict[utag] = len(tag_list)\n",
    "    \n",
    "print(tag_count_dict)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### List of Unknown Words in Validation Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of unknown words in validation data set : 335\n"
     ]
    }
   ],
   "source": [
    "val_data_unknown_words = [word for sent in validation_set for (word, tag) in sent if word not in vocabulary]\n",
    "print('number of unknown words in validation data set : {0}'.format(len(set(val_data_unknown_words))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### List of Unknown Words in Test Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of unknown words in test data set : 28\n"
     ]
    }
   ],
   "source": [
    "test_data_unknown_words = [word for word in test_data_words if word not in vocabulary]\n",
    "print('number of unknown words in test data set : {0}'.format(len(set(test_data_unknown_words))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate Number of Words correctly tagged in Test Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_test_dataset_accuracy(tagged_test_set):\n",
    "    total_words = 0\n",
    "    correct_tagged_words = 0\n",
    "\n",
    "    for word, tag in tagged_test_set:\n",
    "        try:\n",
    "            list_for_tag = test_tagged_words[tag]\n",
    "        except KeyError:\n",
    "            list_for_tag = []\n",
    "\n",
    "        total_words += 1\n",
    "\n",
    "        if word in list_for_tag:\n",
    "            correct_tagged_words += 1\n",
    "\n",
    "    print('total words - {0}. correctly tagged words - {1}. accuracy - {2}'.\n",
    "          format(total_words, correct_tagged_words,\n",
    "                 correct_tagged_words / total_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Learning HMM Model Parameters\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Emission Probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_given_tag(word, tag, train_bag=train_tagged_words):\n",
    "\n",
    "    w_given_tag_list = [\n",
    "        pair[0] for pair in train_bag if pair[0] == word and pair[1] == tag\n",
    "    ]\n",
    "    count_w_given_tag = len(w_given_tag_list)\n",
    "\n",
    "    return count_w_given_tag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transition Probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def t2_given_t1(t2, t1, train_bag=train_tagged_words):\n",
    "    \n",
    "    count_t2_t1 = 0\n",
    "\n",
    "    for index in range(len(all_tags) - 1):\n",
    "        if all_tags[index] == t1 and all_tags[index + 1] == t2:\n",
    "            count_t2_t1 += 1\n",
    "\n",
    "    return count_t2_t1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "tags_matrix = np.zeros((len(unique_tags), len(unique_tags)), dtype='float32')\n",
    "\n",
    "for i, t1 in enumerate(list(unique_tags)):\n",
    "    for j, t2 in enumerate(list(unique_tags)):\n",
    "        count_t1 = tag_count_dict[t1]\n",
    "        tags_matrix[i, j] = t2_given_t1(t2, t1) / count_t1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>.</th>\n",
       "      <th>ADJ</th>\n",
       "      <th>ADP</th>\n",
       "      <th>ADV</th>\n",
       "      <th>CONJ</th>\n",
       "      <th>DET</th>\n",
       "      <th>NOUN</th>\n",
       "      <th>NUM</th>\n",
       "      <th>PRON</th>\n",
       "      <th>PRT</th>\n",
       "      <th>VERB</th>\n",
       "      <th>X</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>.</th>\n",
       "      <td>0.094070</td>\n",
       "      <td>0.044654</td>\n",
       "      <td>0.090386</td>\n",
       "      <td>0.051932</td>\n",
       "      <td>0.057772</td>\n",
       "      <td>0.173226</td>\n",
       "      <td>0.223091</td>\n",
       "      <td>0.080593</td>\n",
       "      <td>0.065768</td>\n",
       "      <td>0.002336</td>\n",
       "      <td>0.088769</td>\n",
       "      <td>0.027314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ADJ</th>\n",
       "      <td>0.065809</td>\n",
       "      <td>0.065314</td>\n",
       "      <td>0.077519</td>\n",
       "      <td>0.004948</td>\n",
       "      <td>0.016658</td>\n",
       "      <td>0.004948</td>\n",
       "      <td>0.698499</td>\n",
       "      <td>0.021112</td>\n",
       "      <td>0.000660</td>\n",
       "      <td>0.010886</td>\n",
       "      <td>0.012205</td>\n",
       "      <td>0.021442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ADP</th>\n",
       "      <td>0.039842</td>\n",
       "      <td>0.105785</td>\n",
       "      <td>0.016512</td>\n",
       "      <td>0.013849</td>\n",
       "      <td>0.000959</td>\n",
       "      <td>0.322893</td>\n",
       "      <td>0.322893</td>\n",
       "      <td>0.062001</td>\n",
       "      <td>0.070203</td>\n",
       "      <td>0.001491</td>\n",
       "      <td>0.008522</td>\n",
       "      <td>0.035048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ADV</th>\n",
       "      <td>0.134666</td>\n",
       "      <td>0.129751</td>\n",
       "      <td>0.118611</td>\n",
       "      <td>0.081258</td>\n",
       "      <td>0.006881</td>\n",
       "      <td>0.068480</td>\n",
       "      <td>0.031127</td>\n",
       "      <td>0.031455</td>\n",
       "      <td>0.015400</td>\n",
       "      <td>0.014744</td>\n",
       "      <td>0.344364</td>\n",
       "      <td>0.023263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CONJ</th>\n",
       "      <td>0.033116</td>\n",
       "      <td>0.118937</td>\n",
       "      <td>0.052705</td>\n",
       "      <td>0.055970</td>\n",
       "      <td>0.000466</td>\n",
       "      <td>0.118470</td>\n",
       "      <td>0.348881</td>\n",
       "      <td>0.041511</td>\n",
       "      <td>0.057369</td>\n",
       "      <td>0.005131</td>\n",
       "      <td>0.158582</td>\n",
       "      <td>0.008862</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DET</th>\n",
       "      <td>0.017777</td>\n",
       "      <td>0.203652</td>\n",
       "      <td>0.009191</td>\n",
       "      <td>0.012698</td>\n",
       "      <td>0.000484</td>\n",
       "      <td>0.005442</td>\n",
       "      <td>0.638650</td>\n",
       "      <td>0.022373</td>\n",
       "      <td>0.003749</td>\n",
       "      <td>0.000242</td>\n",
       "      <td>0.039545</td>\n",
       "      <td>0.046197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NOUN</th>\n",
       "      <td>0.239307</td>\n",
       "      <td>0.012231</td>\n",
       "      <td>0.177023</td>\n",
       "      <td>0.017182</td>\n",
       "      <td>0.042263</td>\n",
       "      <td>0.013250</td>\n",
       "      <td>0.264898</td>\n",
       "      <td>0.009537</td>\n",
       "      <td>0.004769</td>\n",
       "      <td>0.043974</td>\n",
       "      <td>0.146336</td>\n",
       "      <td>0.029231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NUM</th>\n",
       "      <td>0.115933</td>\n",
       "      <td>0.032402</td>\n",
       "      <td>0.035672</td>\n",
       "      <td>0.002973</td>\n",
       "      <td>0.013377</td>\n",
       "      <td>0.002973</td>\n",
       "      <td>0.354637</td>\n",
       "      <td>0.184899</td>\n",
       "      <td>0.001486</td>\n",
       "      <td>0.027051</td>\n",
       "      <td>0.018133</td>\n",
       "      <td>0.210464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PRON</th>\n",
       "      <td>0.040473</td>\n",
       "      <td>0.073692</td>\n",
       "      <td>0.023291</td>\n",
       "      <td>0.032837</td>\n",
       "      <td>0.004582</td>\n",
       "      <td>0.009164</td>\n",
       "      <td>0.207331</td>\n",
       "      <td>0.007255</td>\n",
       "      <td>0.007637</td>\n",
       "      <td>0.011837</td>\n",
       "      <td>0.487972</td>\n",
       "      <td>0.093929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PRT</th>\n",
       "      <td>0.041694</td>\n",
       "      <td>0.084039</td>\n",
       "      <td>0.021173</td>\n",
       "      <td>0.009772</td>\n",
       "      <td>0.002280</td>\n",
       "      <td>0.099674</td>\n",
       "      <td>0.247883</td>\n",
       "      <td>0.056678</td>\n",
       "      <td>0.017915</td>\n",
       "      <td>0.001954</td>\n",
       "      <td>0.402932</td>\n",
       "      <td>0.014007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>VERB</th>\n",
       "      <td>0.035167</td>\n",
       "      <td>0.065221</td>\n",
       "      <td>0.091402</td>\n",
       "      <td>0.083501</td>\n",
       "      <td>0.005577</td>\n",
       "      <td>0.133617</td>\n",
       "      <td>0.110844</td>\n",
       "      <td>0.022696</td>\n",
       "      <td>0.035321</td>\n",
       "      <td>0.031216</td>\n",
       "      <td>0.167622</td>\n",
       "      <td>0.217816</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>X</th>\n",
       "      <td>0.162816</td>\n",
       "      <td>0.016456</td>\n",
       "      <td>0.144937</td>\n",
       "      <td>0.026108</td>\n",
       "      <td>0.010759</td>\n",
       "      <td>0.054114</td>\n",
       "      <td>0.062184</td>\n",
       "      <td>0.002690</td>\n",
       "      <td>0.056329</td>\n",
       "      <td>0.184652</td>\n",
       "      <td>0.204114</td>\n",
       "      <td>0.074842</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             .       ADJ       ADP       ADV      CONJ       DET      NOUN  \\\n",
       ".     0.094070  0.044654  0.090386  0.051932  0.057772  0.173226  0.223091   \n",
       "ADJ   0.065809  0.065314  0.077519  0.004948  0.016658  0.004948  0.698499   \n",
       "ADP   0.039842  0.105785  0.016512  0.013849  0.000959  0.322893  0.322893   \n",
       "ADV   0.134666  0.129751  0.118611  0.081258  0.006881  0.068480  0.031127   \n",
       "CONJ  0.033116  0.118937  0.052705  0.055970  0.000466  0.118470  0.348881   \n",
       "DET   0.017777  0.203652  0.009191  0.012698  0.000484  0.005442  0.638650   \n",
       "NOUN  0.239307  0.012231  0.177023  0.017182  0.042263  0.013250  0.264898   \n",
       "NUM   0.115933  0.032402  0.035672  0.002973  0.013377  0.002973  0.354637   \n",
       "PRON  0.040473  0.073692  0.023291  0.032837  0.004582  0.009164  0.207331   \n",
       "PRT   0.041694  0.084039  0.021173  0.009772  0.002280  0.099674  0.247883   \n",
       "VERB  0.035167  0.065221  0.091402  0.083501  0.005577  0.133617  0.110844   \n",
       "X     0.162816  0.016456  0.144937  0.026108  0.010759  0.054114  0.062184   \n",
       "\n",
       "           NUM      PRON       PRT      VERB         X  \n",
       ".     0.080593  0.065768  0.002336  0.088769  0.027314  \n",
       "ADJ   0.021112  0.000660  0.010886  0.012205  0.021442  \n",
       "ADP   0.062001  0.070203  0.001491  0.008522  0.035048  \n",
       "ADV   0.031455  0.015400  0.014744  0.344364  0.023263  \n",
       "CONJ  0.041511  0.057369  0.005131  0.158582  0.008862  \n",
       "DET   0.022373  0.003749  0.000242  0.039545  0.046197  \n",
       "NOUN  0.009537  0.004769  0.043974  0.146336  0.029231  \n",
       "NUM   0.184899  0.001486  0.027051  0.018133  0.210464  \n",
       "PRON  0.007255  0.007637  0.011837  0.487972  0.093929  \n",
       "PRT   0.056678  0.017915  0.001954  0.402932  0.014007  \n",
       "VERB  0.022696  0.035321  0.031216  0.167622  0.217816  \n",
       "X     0.002690  0.056329  0.184652  0.204114  0.074842  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tag = pd.DataFrame(tags_matrix,\n",
    "                      columns=list(unique_tags),\n",
    "                      index=list(unique_tags))\n",
    "\n",
    "df_tag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Start Probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       ".       0.094070\n",
       "ADJ     0.044654\n",
       "ADP     0.090386\n",
       "ADV     0.051932\n",
       "CONJ    0.057772\n",
       "DET     0.173226\n",
       "NOUN    0.223091\n",
       "NUM     0.080593\n",
       "PRON    0.065768\n",
       "PRT     0.002336\n",
       "VERB    0.088769\n",
       "X       0.027314\n",
       "Name: ., dtype: float32"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tag.loc['.', :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Vanilla Viterbi Based POS Tagger\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Viterbi_Vanilla(words, train_bag=train_tagged_words):\n",
    "    state = []\n",
    "    \n",
    "    for key, word in enumerate(words):\n",
    "        #initialise list of probability column for a given observation\n",
    "        p = []\n",
    "        for tag in unique_tags:\n",
    "            if key == 0:\n",
    "                transition_p = df_tag.loc['.', tag]\n",
    "            else:\n",
    "                transition_p = df_tag.loc[state[-1], tag]\n",
    "\n",
    "            # compute emission and state probabilities\n",
    "            emission_p = word_given_tag(words[key], tag) / tag_count_dict[tag]\n",
    "            state_probability = emission_p * transition_p\n",
    "            \n",
    "            p.append(state_probability)\n",
    "\n",
    "        pmax = max(p)\n",
    "        \n",
    "        # getting state for which probability is maximum\n",
    "        # tagging unknown words as 'X' to mark those as foreign words\n",
    "        if pmax == 0:\n",
    "            state_max = 'X'\n",
    "        else:    \n",
    "            state_max = unique_tags[p.index(pmax)]\n",
    "\n",
    "        state.append(state_max)\n",
    "\n",
    "    return list(zip(words, state))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Running Algorithm On Validation Dataset\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(1234)\n",
    "\n",
    "random_indices = [random.randint(1, len(validation_set)) for x in range(5)]\n",
    "\n",
    "validation_run = [validation_set[i] for i in random_indices]\n",
    "\n",
    "validation_run_base = [tup for sent in validation_run for tup in sent]\n",
    "\n",
    "validation_untagged_words = [tup[0] for tup in validation_run_base]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of words in selected validation set : 166\n"
     ]
    }
   ],
   "source": [
    "print('number of words in selected validation set : {0}'.format(len(validation_untagged_words)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 20.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "validation_tagged_sent = Viterbi_Vanilla(validation_untagged_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Model Validation\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8674698795180723"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correct_tags = [i for i, j in zip(validation_run_base, validation_tagged_sent) if i == j]\n",
    "\n",
    "accuracy = len(correct_tags) / len(validation_run_base)\n",
    "\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(('sell', 'NOUN'), ('sell', 'VERB')),\n",
       " (('printers', 'NOUN'), ('printers', 'X')),\n",
       " (('there', 'ADV'), ('there', 'DET')),\n",
       " (('Gunmen', 'NOUN'), ('Gunmen', 'X')),\n",
       " (('Lebanon', 'NOUN'), ('Lebanon', 'X')),\n",
       " (('assassinated', 'VERB'), ('assassinated', 'X')),\n",
       " (('Arabian', 'NOUN'), ('Arabian', 'X')),\n",
       " (('pro-Iranian', 'ADJ'), ('pro-Iranian', 'X')),\n",
       " (('Islamic', 'NOUN'), ('Islamic', 'X')),\n",
       " (('slaying', 'NOUN'), ('slaying', 'X')),\n",
       " (('avenge', 'VERB'), ('avenge', 'X')),\n",
       " (('beheading', 'NOUN'), ('beheading', 'X')),\n",
       " (('terrorists', 'NOUN'), ('terrorists', 'X')),\n",
       " (('Riyadh', 'NOUN'), ('Riyadh', 'X')),\n",
       " (('Card', 'NOUN'), ('Card', 'X')),\n",
       " (('sweepstakes', 'NOUN'), ('sweepstakes', 'X')),\n",
       " (('forthcoming', 'ADJ'), ('forthcoming', 'X')),\n",
       " (('10-year', 'NUM'), ('10-year', 'ADJ')),\n",
       " (('yen-denominated', 'ADJ'), ('yen-denominated', 'X')),\n",
       " (('about', 'ADV'), ('about', 'ADP')),\n",
       " (('redeeming', 'VERB'), ('redeeming', 'X')),\n",
       " (('convert', 'VERB'), ('convert', 'X'))]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation_incorrect_tagged_words = [(i, j) for i, j in zip(validation_run_base, validation_tagged_sent) if i != j]\n",
    "\n",
    "print(len(validation_incorrect_tagged_words))\n",
    "validation_incorrect_tagged_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> -  *Accuracy of Vanilla Viterbi - in **high 80s** depending on validation dataset*\n",
    "-  *Number of incorrect tagged words from validation set : **22** words*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Running Algorithm On Test Dataset\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 27.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "test_tagged_set = Viterbi_Vanilla(test_data_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate Test Dataset Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total words - 181. correctly tagged words - 139. accuracy - 0.7679558011049724\n"
     ]
    }
   ],
   "source": [
    "calc_test_dataset_accuracy(test_tagged_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Android', 'X'), ('Google', 'X'), ('Android', 'X'), ('OS', 'X'), ('worldwide', 'X'), ('smartphones', 'X'), ('2011', 'X'), ('2013', 'X'), ('Google', 'X'), ('Twitter', 'X'), ('2015', 'X'), ('Google', 'X'), ('Twitter', 'X'), ('firehose', 'X'), ('Twitter', 'X'), ('online', 'X'), ('interact', 'X'), ('messages', 'X'), ('tweets', 'X'), ('domineering', 'X'), ('personality', 'X'), ('2018', 'X'), ('FIFA', 'X'), ('Cup', 'X'), ('21st', 'X'), ('FIFA', 'X'), ('Cup', 'X'), ('tournament', 'X'), ('contested', 'X'), ('Cup', 'X'), ('trips', 'X'), ('arriving', 'X'), ('NASA', 'X'), ('invited', 'X'), ('ICESAT-2', 'X'), ('Satellite', 'X')]\n"
     ]
    }
   ],
   "source": [
    "test_unknown_tagged_words = [tup for tup in test_tagged_set if tup[0] in test_data_unknown_words]\n",
    "\n",
    "print(test_unknown_tagged_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> -  *all unknown words have been assigned the <b>1st tag</b> present in the universal tagset*\n",
    "-  *all unknown proper nouns like <b>Android</b> and <b>Google</b> are incorrectly tagged*\n",
    "-  *all unknown numbers like <b>2013</b> and <b>2015</b> are incorrectly tagged*\n",
    "-  *all unknown verbs like <b>contested</b> and <b>arriving</b> are incorrectly tagged*\n",
    "\n",
    "> -  *overall accuracy obtained on test data set : <b>76.79%</b>*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
